{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"room_id\": \"300001\", \"room_created_at\": \"2024-10-22T04:42:30\", \"channel\": \"ads\", \"customer\": {\"customer_id\": \"cust_001\", \"customer_name\": \"Alice Johnson\", \"phone\": \"081234567891\"}, \"messages\": [{\"message_id\": \"msg_001\", \"sender_type\": \"customer\", \"message_text\": \"Hi! I'm interested in swimming classes\", \"message_date\": \"2024-10-22T04:42:30\"}, {\"message_id\": \"msg_002\", \"sender_type\": \"agent\", \"message_text\": \"Sure! We have classes available for 3-5 Y.O.\", \"message_date\": \"2024-10-22T04:45:30\"}, {\"message_id\": \"msg_003\", \"sender_type\": \"system\", \"message_text\": \"Booking confirmed for 24 Oct\", \"message_date\": \"2024-10-23T10:00:00\"}, {\"message_id\": \"msg_004\", \"sender_type\": \"system\", \"message_text\": \"Payment of IDR 500000 confirmed\", \"message_date\": \"2024-10-24T08:00:00\"}]}\n",
      "{\"room_id\": \"300002\", \"room_created_at\": \"2024-10-22T04:47:25\", \"channel\": \"website\", \"customer\": {\"customer_id\": \"cust_002\", \"customer_name\": \"Bob Smith\", \"phone\": \"081234567892\"}, \"messages\": [{\"message_id\": \"msg_005\", \"sender_type\": \"customer\", \"message_text\": \"Interested in a free trial class\", \"message_date\": \"2024-10-22T04:47:25\"}, {\"message_id\": \"msg_006\", \"sender_type\": \"agent\", \"message_text\": \"We can book you for Friday 25th!\", \"message_date\": \"2024-10-22T04:50:00\"}]}\n",
      "{\"room_id\": \"300003\", \"room_created_at\": \"2024-10-22T04:49:53\", \"channel\": \"campaign\", \"customer\": {\"customer_id\": \"cust_003\", \"customer_name\": \"Chloe Davis\", \"phone\": \"081234567893\"}, \"messages\": [{\"message_id\": \"msg_007\", \"sender_type\": \"customer\", \"message_text\": \"Is there a promo for toddlers?\", \"message_date\": \"2024-10-22T04:49:53\"}]}\n",
      "{\"room_id\": \"300004\", \"room_created_at\": \"2024-10-23T09:15:00\", \"channel\": \"organic\", \"customer\": {\"customer_id\": \"cust_004\", \"customer_name\": \"Daniela Fern\", \"phone\": \"081234567894\"}, \"messages\": [{\"message_id\": \"msg_008\", \"sender_type\": \"customer\", \"message_text\": \"How to register for a trial class?\", \"message_date\": \"2024-10-23T09:15:00\"}, {\"message_id\": \"msg_009\", \"sender_type\": \"agent\", \"message_text\": \"You can pick a slot on Saturday morning.\", \"message_date\": \"2024-10-23T09:17:00\"}, {\"message_id\": \"msg_010\", \"sender_type\": \"system\", \"message_text\": \"Booking confirmed for 26 Oct\", \"message_date\": \"2024-10-23T11:00:00\"}]}\n",
      "{\"room_id\": \"300005\", \"room_created_at\": \"2024-10-23T10:30:00\", \"channel\": \"direct\", \"customer\": {\"customer_id\": \"cust_001\", \"customer_name\": \"Alice Johnson\", \"phone\": \"081234567891\"}, \"messages\": [{\"message_id\": \"msg_011\", \"sender_type\": \"customer\", \"message_text\": \"Need class schedule update\", \"message_date\": \"2024-10-23T10:30:00\"}, {\"message_id\": \"msg_012\", \"sender_type\": \"agent\", \"message_text\": \"We've added more morning sessions\", \"message_date\": \"2024-10-23T10:31:00\"}]}\n",
      "{\"room_id\": \"300006\", \"room_created_at\": \"2024-10-23T10:30:00\", \"channel\": \"direct\", \"customer\": {\"customer_id\": \"cust_001\", \"customer_name\": \"Alice Johnson\", \"phone\": \"081234567891\"}, \"messages\": [{\"message_id\": \"msg_021\", \"sender_type\": \"customer\", \"message_text\": \"Need class schedule update\", \"message_date\": \"2024-10-23T10:30:00\"}, {\"message_id\": \"msg_022\", \"sender_type\": \"agent\", \"message_text\": \"We've added more morning sessions\", \"message_date\": \"2024-10-23T10:31:00\"}]}\n",
      "{\"room_id\": \"300007\", \"room_created_at\": \"2025-10-23T10:30:00\", \"channel\": \"direct\", \"customer\": {\"customer_id\": \"cust_001\", \"customer_name\": \"Alice Johnson\", \"phone\": \"081234567891\"}, \"messages\": [{\"message_id\": \"msg_031\", \"sender_type\": \"customer\", \"message_text\": \"Need class schedule update\", \"message_date\": \"2024-10-23T10:30:00\"}, {\"message_id\": \"msg_032\", \"sender_type\": \"agent\", \"message_text\": \"We've added more morning sessions\", \"message_date\": \"2024-10-23T10:31:00\"}]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m      6\u001b[0m consumer \u001b[38;5;241m=\u001b[39m KafkaConsumer(\n\u001b[0;32m      7\u001b[0m     os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOPIC_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# The topic to consume messages from\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     bootstrap_servers\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKAFKA_BROKER\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# List of Kafka brokers to connect to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     value_deserializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Deserialize message values from bytes to UTF-8 strings\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Consume messages with error handling for non-JSON messages\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconsumer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\consumer\\group.py:1203\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\consumer\\group.py:1211\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\consumer\\group.py:1126\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1125\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m-> 1126\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m record_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1128\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[0;32m   1132\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\consumer\\group.py:663\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    661\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m--> 663\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[0;32m    665\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\consumer\\group.py:712\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    711\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m--> 712\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\client_async.py:601\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[1;34m(self, timeout_ms, future)\u001b[0m\n\u001b[0;32m    598\u001b[0m             timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    599\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[0;32m    605\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\client_async.py:633\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[0;32m    632\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 633\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py:323\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    321\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 323\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py:314\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 314\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m select\u001b[38;5;241m.\u001b[39mselect(r, w, w, timeout)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# PostgreSQL connection\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "POSTGRES_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "POSTGRES_PORT = os.getenv(\"POSTGRES_PORT\", \"5435\")\n",
    "POSTGRES_DB = os.getenv(\"POSTGRES_DB\")\n",
    "\n",
    "# SQLAlchemy engine\n",
    "engine = create_engine(f'postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}')\n",
    "\n",
    "\n",
    "# Define Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    os.getenv(\"TOPIC_NAME\"),  # The topic to consume messages from\n",
    "    bootstrap_servers=os.getenv(\"KAFKA_BROKER\"),  # List of Kafka brokers to connect to\n",
    "    auto_offset_reset='earliest',  # Where to start reading messages when no offset is stored ('earliest' to read from the beginning)\n",
    "    enable_auto_commit=True,  # Automatically commit offsets after consuming messages\n",
    "    value_deserializer=lambda x: x.decode('utf-8') if x else None  # Deserialize message values from bytes to UTF-8 strings\n",
    ")\n",
    "\n",
    "# Consume messages with error handling for non-JSON messages\n",
    "for message in consumer:\n",
    "    try:\n",
    "        data = json.loads(msg.value)\n",
    "\n",
    "        room_id = data.get('room_id')\n",
    "        room_created_at = data.get('room_created_at')\n",
    "        channel = data.get('channel')\n",
    "        customer = data.get('customer', {})\n",
    "        messages = data.get('messages', [])\n",
    "\n",
    "        rows = []\n",
    "        for message in messages:\n",
    "            rows.append({\n",
    "                \"message_id\": message.get(\"message_id\"),\n",
    "                \"room_id\": room_id,\n",
    "                \"room_created_at\": room_created_at,\n",
    "                \"channel\": channel,\n",
    "                \"customer_id\": customer.get(\"customer_id\"),\n",
    "                \"customer_name\": customer.get(\"customer_name\"),\n",
    "                \"phone\": customer.get(\"phone\"),\n",
    "                \"sender_type\": message.get(\"sender_type\"),\n",
    "                \"message_text\": message.get(\"message_text\"),\n",
    "                \"message_date\": message.get(\"message_date\")\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "        # Insert into Postgres\n",
    "        df.to_sql(\"fact_message\", engine, if_exists='append', index=False)\n",
    "\n",
    "        print(f\"Inserted {len(df)} messages into Postgres.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing message: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f99fd2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting Chat Data Processor...\n",
      "INFO:__main__:Topic: chat_msgs\n",
      "INFO:__main__:Kafka Broker: localhost:9092\n",
      "INFO:__main__:PostgreSQL Database: messages_db\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.6.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup\n",
      "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ('chat_msgs',)\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='chat_msgs', partition=0)]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. \n",
      "INFO:__main__:Received interrupt signal, processing remaining messages...\n",
      "ERROR:__main__:Failed to process batch: ON CONFLICT DO UPDATE command cannot affect row a second time\n",
      "HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.\n",
      "\n",
      "ERROR:__main__:Processor failed: ON CONFLICT DO UPDATE command cannot affect row a second time\n",
      "HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.\n",
      "\n",
      "INFO:kafka.conn:<BrokerConnection node_id=1001 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. \n",
      "ERROR:kafka.consumer.fetcher:Fetch to node 1001 failed: Cancelled: <BrokerConnection node_id=1001 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>\n",
      "INFO:__main__:Connections closed successfully\n"
     ]
    },
    {
     "ename": "CardinalityViolation",
     "evalue": "ON CONFLICT DO UPDATE command cannot affect row a second time\nHINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 191\u001b[0m, in \u001b[0;36mprocess_messages_batch\u001b[1;34m(consumer, pg_connection, batch_size)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconsumer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Handle both single room object and array of rooms\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\consumer\\group.py:1203\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\consumer\\group.py:1211\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\consumer\\group.py:1126\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1125\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m-> 1126\u001b[0m record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m record_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\consumer\\group.py:663\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m--> 663\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\consumer\\group.py:712\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    711\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m--> 712\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\client_async.py:601\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[1;34m(self, timeout_ms, future)\u001b[0m\n\u001b[0;32m    599\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kafka\\client_async.py:633\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    632\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 633\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py:323\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 323\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py:314\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 314\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m select\u001b[38;5;241m.\u001b[39mselect(r, w, w, timeout)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCardinalityViolation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 272\u001b[0m\n\u001b[0;32m    269\u001b[0m         close_connections(consumer, pg_connection)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 272\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 262\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    259\u001b[0m     pg_connection \u001b[38;5;241m=\u001b[39m create_pg_connection()\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# Start processing messages\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     \u001b[43mprocess_messages_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconsumer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpg_connection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    265\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessor failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 211\u001b[0m, in \u001b[0;36mprocess_messages_batch\u001b[1;34m(consumer, pg_connection, batch_size)\u001b[0m\n\u001b[0;32m    209\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived interrupt signal, processing remaining messages...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m message_batch:\n\u001b[1;32m--> 211\u001b[0m         \u001b[43mprocess_chat_data_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpg_connection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing messages: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 132\u001b[0m, in \u001b[0;36mprocess_chat_data_batch\u001b[1;34m(rooms_batch, pg_connection)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Insert customers (with conflict resolution)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m customers_data:\n\u001b[1;32m--> 132\u001b[0m     \u001b[43mexecute_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250;43m        \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;43;03m        INSERT INTO stream.customers (customer_id, customer_name, phone) \u001b[39;49;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;43;03m        VALUES %s \u001b[39;49;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;43;03m        ON CONFLICT (customer_id) \u001b[39;49;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;43;03m        DO UPDATE SET \u001b[39;49;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;43;03m            customer_name = EXCLUDED.customer_name,\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;43;03m            phone = EXCLUDED.phone,\u001b[39;49;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;43;03m            updated_at = CURRENT_TIMESTAMP\u001b[39;49;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;43;03m        \"\"\"\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustomers_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Insert rooms (with conflict resolution)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rooms_data:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\psycopg2\\extras.py:1299\u001b[0m, in \u001b[0;36mexecute_values\u001b[1;34m(cur, sql, argslist, template, page_size, fetch)\u001b[0m\n\u001b[0;32m   1297\u001b[0m     parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1298\u001b[0m parts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m post\n\u001b[1;32m-> 1299\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fetch:\n\u001b[0;32m   1301\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(cur\u001b[38;5;241m.\u001b[39mfetchall())\n",
      "\u001b[1;31mCardinalityViolation\u001b[0m: ON CONFLICT DO UPDATE command cannot affect row a second time\nHINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from kafka import KafkaConsumer\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def deserialize_json(x):\n",
    "    \"\"\"Deserialize message value to JSON\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(x.decode('utf-8'))\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to decode JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_kafka_consumer():\n",
    "    \"\"\"Create Kafka consumer with JSON deserialization\"\"\"\n",
    "    return KafkaConsumer(\n",
    "        os.getenv(\"TOPIC_NAME\"),\n",
    "        bootstrap_servers=os.getenv(\"KAFKA_BROKER\"),\n",
    "        auto_offset_reset='earliest',\n",
    "        enable_auto_commit=True,\n",
    "        value_deserializer=deserialize_json\n",
    "    )\n",
    "\n",
    "def create_pg_connection():\n",
    "    \"\"\"Create PostgreSQL connection\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            host='localhost',\n",
    "            database='streaming',\n",
    "            user='streaming',\n",
    "            password='password',\n",
    "            port=5435\n",
    "        )\n",
    "        connection.autocommit = False  # We'll handle transactions manually\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_room_data(room):\n",
    "    \"\"\"Validate room data structure\"\"\"\n",
    "    required_fields = ['room_id', 'customer']\n",
    "    for field in required_fields:\n",
    "        if field not in room:\n",
    "            logger.warning(f\"Missing required field '{field}' in room data\")\n",
    "            return False\n",
    "    \n",
    "    if 'customer_id' not in room.get('customer', {}):\n",
    "        logger.warning(\"Missing customer_id in customer data\")\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def parse_datetime(date_str):\n",
    "    \"\"\"Parse datetime string to datetime object\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Handle different datetime formats\n",
    "        for fmt in ['%Y-%m-%dT%H:%M:%S', '%Y-%m-%d %H:%M:%S']:\n",
    "            try:\n",
    "                return datetime.strptime(date_str, fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        logger.warning(f\"Could not parse datetime: {date_str}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing datetime {date_str}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_chat_data_batch(rooms_batch, pg_connection):\n",
    "    \"\"\"Process a batch of room data and insert into normalized tables\"\"\"\n",
    "    if not rooms_batch:\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        cursor = pg_connection.cursor()\n",
    "        \n",
    "        # Prepare data for batch insertion\n",
    "        customers_data = []\n",
    "        rooms_data = []\n",
    "        messages_data = []\n",
    "        \n",
    "        # Extract and prepare data\n",
    "        for room in rooms_batch:\n",
    "            if not validate_room_data(room):\n",
    "                continue\n",
    "            \n",
    "            # Extract customer data\n",
    "            customer = room.get('customer', {})\n",
    "            customer_record = (\n",
    "                customer.get('customer_id'),\n",
    "                customer.get('customer_name'),\n",
    "                customer.get('phone')\n",
    "            )\n",
    "            customers_data.append(customer_record)\n",
    "            \n",
    "            # Extract room data\n",
    "            room_record = (\n",
    "                room.get('room_id'),\n",
    "                customer.get('customer_id'),\n",
    "                room.get('channel'),\n",
    "                parse_datetime(room.get('room_created_at'))\n",
    "            )\n",
    "            rooms_data.append(room_record)\n",
    "            \n",
    "            # Extract messages data\n",
    "            for message in room.get('messages', []):\n",
    "                message_record = (\n",
    "                    message.get('message_id'),\n",
    "                    room.get('room_id'),\n",
    "                    message.get('sender_type'),\n",
    "                    message.get('message_text'),\n",
    "                    parse_datetime(message.get('message_date'))\n",
    "                )\n",
    "                messages_data.append(message_record)\n",
    "        \n",
    "        # Insert customers (with conflict resolution)\n",
    "        if customers_data:\n",
    "            execute_values(\n",
    "                cursor,\n",
    "                \"\"\"\n",
    "                INSERT INTO stream.customers (customer_id, customer_name, phone) \n",
    "                VALUES %s \n",
    "                ON CONFLICT (customer_id) \n",
    "                DO UPDATE SET \n",
    "                    customer_name = EXCLUDED.customer_name,\n",
    "                    phone = EXCLUDED.phone,\n",
    "                    updated_at = CURRENT_TIMESTAMP\n",
    "                \"\"\",\n",
    "                customers_data,\n",
    "                template=None,\n",
    "                page_size=100\n",
    "            )\n",
    "        \n",
    "        # Insert rooms (with conflict resolution)\n",
    "        if rooms_data:\n",
    "            execute_values(\n",
    "                cursor,\n",
    "                \"\"\"\n",
    "                INSERT INTO stream.rooms (room_id, customer_id, channel, room_created_at) \n",
    "                VALUES %s \n",
    "                ON CONFLICT (room_id) DO NOTHING\n",
    "                \"\"\",\n",
    "                rooms_data,\n",
    "                template=None,\n",
    "                page_size=100\n",
    "            )\n",
    "        \n",
    "        # Insert messages (with conflict resolution)\n",
    "        if messages_data:\n",
    "            execute_values(\n",
    "                cursor,\n",
    "                \"\"\"\n",
    "                INSERT INTO stream.messages (message_id, room_id, sender_type, message_text, message_date) \n",
    "                VALUES %s \n",
    "                ON CONFLICT (message_id) DO NOTHING\n",
    "                \"\"\",\n",
    "                messages_data,\n",
    "                template=None,\n",
    "                page_size=100\n",
    "            )\n",
    "        \n",
    "        pg_connection.commit()\n",
    "        \n",
    "        logger.info(f\"Successfully processed batch: {len(customers_data)} customers, \"\n",
    "                   f\"{len(rooms_data)} rooms, {len(messages_data)} messages\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process batch: {e}\")\n",
    "        pg_connection.rollback()\n",
    "        raise\n",
    "\n",
    "def process_messages_batch(consumer, pg_connection, batch_size=50):\n",
    "    \"\"\"Process messages in batches for better performance\"\"\"\n",
    "    message_batch = []\n",
    "    \n",
    "    try:\n",
    "        for message in consumer:\n",
    "            if message.value is not None:\n",
    "                # Handle both single room object and array of rooms\n",
    "                rooms_data = message.value\n",
    "                if isinstance(rooms_data, dict):\n",
    "                    rooms_data = [rooms_data]  # Convert single object to array\n",
    "                elif not isinstance(rooms_data, list):\n",
    "                    logger.warning(f\"Unexpected data format: {type(rooms_data)}\")\n",
    "                    continue\n",
    "                \n",
    "                message_batch.extend(rooms_data)\n",
    "                \n",
    "                # Process batch when it reaches the specified size\n",
    "                if len(message_batch) >= batch_size:\n",
    "                    process_chat_data_batch(message_batch, pg_connection)\n",
    "                    message_batch = []\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Received interrupt signal, processing remaining messages...\")\n",
    "        if message_batch:\n",
    "            process_chat_data_batch(message_batch, pg_connection)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing messages: {e}\")\n",
    "        raise\n",
    "\n",
    "def close_connections(consumer, pg_connection):\n",
    "    \"\"\"Close all connections\"\"\"\n",
    "    try:\n",
    "        if consumer:\n",
    "            consumer.close()\n",
    "        if pg_connection:\n",
    "            pg_connection.close()\n",
    "        logger.info(\"Connections closed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error closing connections: {e}\")\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"Validate required environment variables\"\"\"\n",
    "    required_env_vars = [\n",
    "        \"TOPIC_NAME\", \"KAFKA_BROKER\", \"POSTGRES_DB\", \n",
    "        \"POSTGRES_USER\", \"POSTGRES_PASSWORD\"\n",
    "    ]\n",
    "    \n",
    "    missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "    if missing_vars:\n",
    "        logger.error(f\"Missing required environment variables: {missing_vars}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the Chat Data Processor\"\"\"\n",
    "    \n",
    "    # Validate environment variables\n",
    "    if not validate_environment():\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Starting Chat Data Processor...\")\n",
    "    logger.info(f\"Topic: {os.getenv('TOPIC_NAME')}\")\n",
    "    logger.info(f\"Kafka Broker: {os.getenv('KAFKA_BROKER')}\")\n",
    "    logger.info(f\"PostgreSQL Database: {os.getenv('POSTGRES_DB')}\")\n",
    "    \n",
    "    # Initialize connections\n",
    "    consumer = None\n",
    "    pg_connection = None\n",
    "    \n",
    "    try:\n",
    "        # Create connections\n",
    "        consumer = create_kafka_consumer()\n",
    "        pg_connection = create_pg_connection()\n",
    "         \n",
    "        # Start processing messages\n",
    "        process_messages_batch(consumer, pg_connection)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processor failed: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Clean up connections\n",
    "        close_connections(consumer, pg_connection)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analytics_queries():\n",
    "    \"\"\"Return useful analytics queries for the chat data\"\"\"\n",
    "    return {\n",
    "        \"messages_per_channel\": \"\"\"\n",
    "            SELECT r.channel, COUNT(m.message_id) as message_count\n",
    "            FROM rooms r\n",
    "            LEFT JOIN messages m ON r.room_id = m.room_id\n",
    "            GROUP BY r.channel\n",
    "            ORDER BY message_count DESC;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"customer_activity\": \"\"\"\n",
    "            SELECT c.customer_name, c.phone, \n",
    "                   COUNT(DISTINCT r.room_id) as total_rooms,\n",
    "                   COUNT(m.message_id) as total_messages\n",
    "            FROM customers c\n",
    "            LEFT JOIN rooms r ON c.customer_id = r.customer_id\n",
    "            LEFT JOIN messages m ON r.room_id = m.room_id\n",
    "            GROUP BY c.customer_id, c.customer_name, c.phone\n",
    "            ORDER BY total_messages DESC;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"daily_message_volume\": \"\"\"\n",
    "            SELECT DATE(m.message_date) as message_date,\n",
    "                   m.sender_type,\n",
    "                   COUNT(*) as message_count\n",
    "            FROM messages m\n",
    "            WHERE m.message_date IS NOT NULL\n",
    "            GROUP BY DATE(m.message_date), m.sender_type\n",
    "            ORDER BY message_date DESC, sender_type;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"recent_conversations\": \"\"\"\n",
    "            SELECT r.room_id, c.customer_name, r.channel,\n",
    "                   r.room_created_at, COUNT(m.message_id) as message_count,\n",
    "                   MAX(m.message_date) as last_message_date\n",
    "            FROM rooms r\n",
    "            JOIN customers c ON r.customer_id = c.customer_id\n",
    "            LEFT JOIN messages m ON r.room_id = m.room_id\n",
    "            GROUP BY r.room_id, c.customer_name, r.channel, r.room_created_at\n",
    "            ORDER BY last_message_date DESC\n",
    "            LIMIT 20;\n",
    "        \"\"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94f216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28471695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d84888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36432305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
